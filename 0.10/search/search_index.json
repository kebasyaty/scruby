{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Scruby","text":"<p> A fast key-value storage library. <p> </p> </p> <p></p> <p> Scruby is a fast key-value storage asynchronous library that provides an ordered mapping from string keys to string values. The library uses fractal-tree addressing.   The database consists of collections. The maximum size of the one collection is 16\\*\\*8=4294967296 branches, each branch can store one or more keys.   The value of any key in collection can be obtained in 8 steps, thereby achieving high performance.   In the future, to search by value of key, the use of a quantum loop is supposed. </p> <p></p>"},{"location":"#requirements","title":"Requirements","text":"<p>View the list of requirements.</p>"},{"location":"#changelog","title":"Changelog","text":"<p>View the change history.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT.</p>"},{"location":"pages/constants/","title":"Constants","text":"<p>Constant variables.</p> <p>The module contains the following variables:</p> <ul> <li><code>DB_ROOT</code> - Path to root directory of database. <code>By default = \"ScrubyDB\"</code> (in root of project).</li> <li><code>LENGTH_REDUCTION_HASH</code> - The length of the hash reduction on the left side.<ul> <li><code>0</code> - 4294967296 branches in collection (by default).</li> <li><code>2</code> - 16777216 branches in collection\u044e</li> <li><code>4</code> - 65536 branches in collection\u044e</li> <li><code>6</code> - 256 branches in collection (main purpose is tests).</li> </ul> </li> </ul>"},{"location":"pages/db/","title":"Details","text":"<p>Creation and management of the database.</p>"},{"location":"pages/db/#scruby.db.Scruby","title":"<code>Scruby</code>","text":"<p>Creation and management of database.</p> <p>Parameters:</p> Name Type Description Default <code>class_model</code> <code>T</code> <p>Class of Model (Pydantic).</p> required Source code in <code>src\\scruby\\db.py</code> <pre><code>class Scruby[T]:\n    \"\"\"Creation and management of database.\n\n    Args:\n        class_model: Class of Model (Pydantic).\n    \"\"\"\n\n    def __init__(  # noqa: D107\n        self,\n        class_model: T,\n    ) -&gt; None:\n        self.__meta = _Meta\n        self.__class_model = class_model\n        self.__db_root = constants.DB_ROOT\n        self.__length_reduction_hash = constants.LENGTH_REDUCTION_HASH\n        # The maximum number of keys.\n        match self.__length_reduction_hash:\n            case 0:\n                self.__max_num_keys = 4294967296\n            case 2:\n                self.__max_num_keys = 16777216\n            case 4:\n                self.__max_num_keys = 65536\n            case 6:\n                self.__max_num_keys = 256\n            case _ as unreachable:\n                msg: str = f\"{unreachable} - Unacceptable value for LENGTH_REDUCTION_HASH.\"\n                logger.critical(msg)\n                assert_never(Never(unreachable))\n        # 1.Create metadata if absent.\n        # 2.Check metadata.\n        self._create_metadata()\n\n    def _create_metadata(self) -&gt; None:\n        \"\"\"Create metadata for collection if absent.\n\n        This method is for internal use.\n        \"\"\"\n        key: int = 0\n        key_as_hash: str = f\"{key:08x}\"[self.__length_reduction_hash :]\n        separated_hash: str = \"/\".join(list(key_as_hash))\n        branch_path = SyncPath(\n            *(\n                self.__db_root,\n                self.__class_model.__name__,\n                separated_hash,\n            ),\n        )\n        if not branch_path.exists():\n            branch_path.mkdir(parents=True)\n            meta = _Meta(\n                db_root=self.__db_root,\n                model_name=self.__class_model.__name__,\n                length_reduction_hash=self.__length_reduction_hash,\n                counter_documents=0,\n            )\n            meta_json = meta.model_dump_json()\n            meta_path = SyncPath(*(branch_path, \"meta.json\"))\n            meta_path.write_text(meta_json, \"utf-8\")\n\n    async def _get_meta_path(self) -&gt; Path:\n        \"\"\"Asynchronous method for getting path to metadata of collection.\n\n        This method is for internal use.\n        \"\"\"\n        key: int = 0\n        key_as_hash: str = f\"{key:08x}\"[self.__length_reduction_hash :]\n        separated_hash: str = \"/\".join(list(key_as_hash))\n        return Path(\n            *(\n                self.__db_root,\n                self.__class_model.__name__,\n                separated_hash,\n                \"meta.json\",\n            ),\n        )\n\n    async def _get_meta(self) -&gt; _Meta:\n        \"\"\"Asynchronous method for getting metadata of collection.\n\n        This method is for internal use.\n        \"\"\"\n        meta_path = await self._get_meta_path()\n        meta_json = await meta_path.read_text()\n        meta: _Meta = self.__meta.model_validate_json(meta_json)\n        return meta\n\n    async def _set_meta(self, meta: _Meta) -&gt; None:\n        \"\"\"Asynchronous method for updating metadata of collection.\n\n        This method is for internal use.\n        \"\"\"\n        meta_path = await self._get_meta_path()\n        meta_json = meta.model_dump_json()\n        await meta_path.write_text(meta_json, \"utf-8\")\n\n    async def _counter_documents(self, step: Literal[1, -1]) -&gt; None:\n        \"\"\"Management of documents in metadata of collection.\n\n        This method is for internal use.\n        \"\"\"\n        meta = await self._get_meta()\n        meta.counter_documents += step\n        if meta.counter_documents &lt; 0:\n            meta.counter_documents = 0\n        await self._set_meta(meta)\n\n    async def _get_leaf_path(self, key: str) -&gt; Path:\n        \"\"\"Asynchronous method for getting path to collection cell by key.\n\n        This method is for internal use.\n\n        Args:\n            key: Key name.\n        \"\"\"\n        if not isinstance(key, str):\n            logger.error(\"The key is not a type of `str`.\")\n            raise KeyError(\"The key is not a type of `str`.\")\n        if len(key) == 0:\n            logger.error(\"The key should not be empty.\")\n            raise KeyError(\"The key should not be empty.\")\n        # Key to crc32 sum.\n        key_as_hash: str = f\"{zlib.crc32(key.encode('utf-8')):08x}\"[self.__length_reduction_hash :]\n        # Convert crc32 sum in the segment of path.\n        separated_hash: str = \"/\".join(list(key_as_hash))\n        # The path of the branch to the database.\n        branch_path: Path = Path(\n            *(\n                self.__db_root,\n                self.__class_model.__name__,\n                separated_hash,\n            ),\n        )\n        # If the branch does not exist, need to create it.\n        if not await branch_path.exists():\n            await branch_path.mkdir(parents=True)\n        # The path to the database cell.\n        leaf_path: Path = Path(*(branch_path, \"leaf.json\"))\n        return leaf_path\n\n    async def set_key(\n        self,\n        key: str,\n        value: T,\n    ) -&gt; None:\n        \"\"\"Asynchronous method for adding and updating keys to collection.\n\n        Args:\n            key: Key name.\n            value: Value of key.\n        \"\"\"\n        # The path to the database cell.\n        leaf_path: Path = await self._get_leaf_path(key)\n        value_json: str = value.model_dump_json()\n        # Write key-value to the database.\n        if await leaf_path.exists():\n            # Add new key or update existing.\n            data_json: bytes = await leaf_path.read_bytes()\n            data: dict = orjson.loads(data_json) or {}\n            if data.get(key) is None:\n                await self._counter_documents(1)\n            data[key] = value_json\n            await leaf_path.write_bytes(orjson.dumps(data))\n        else:\n            # Add new key to a blank leaf.\n            await leaf_path.write_bytes(orjson.dumps({key: value_json}))\n            await self._counter_documents(1)\n\n    async def get_key(self, key: str) -&gt; T:\n        \"\"\"Asynchronous method for getting value of key from collection.\n\n        Args:\n            key: Key name.\n        \"\"\"\n        # The path to the database cell.\n        leaf_path: Path = await self._get_leaf_path(key)\n        # Get value of key.\n        if await leaf_path.exists():\n            data_json: bytes = await leaf_path.read_bytes()\n            data: dict = orjson.loads(data_json) or {}\n            obj: T = self.__class_model.model_validate_json(data[key])\n            return obj\n        msg: str = \"`get_key` - The unacceptable key value.\"\n        logger.error(msg)\n        raise KeyError()\n\n    async def has_key(self, key: str) -&gt; bool:\n        \"\"\"Asynchronous method for checking presence of key in collection.\n\n        Args:\n            key: Key name.\n        \"\"\"\n        # The path to the database cell.\n        leaf_path: Path = await self._get_leaf_path(key)\n        # Checking whether there is a key.\n        if await leaf_path.exists():\n            data_json: bytes = await leaf_path.read_bytes()\n            data: dict = orjson.loads(data_json) or {}\n            try:\n                data[key]\n                return True\n            except KeyError:\n                return False\n        return False\n\n    async def delete_key(self, key: str) -&gt; None:\n        \"\"\"Asynchronous method for deleting key from collection.\n\n        Args:\n            key: Key name.\n        \"\"\"\n        # The path to the database cell.\n        leaf_path: Path = await self._get_leaf_path(key)\n        # Deleting key.\n        if await leaf_path.exists():\n            data_json: bytes = await leaf_path.read_bytes()\n            data: dict = orjson.loads(data_json) or {}\n            del data[key]\n            await leaf_path.write_bytes(orjson.dumps(data))\n            await self._counter_documents(-1)\n            return\n        msg: str = \"`delete_key` - The unacceptable key value.\"\n        logger.error(msg)\n        raise KeyError()\n\n    @staticmethod\n    async def napalm() -&gt; None:\n        \"\"\"Asynchronous method for full database deletion.\n\n        The main purpose is tests.\n\n        Warning:\n            - `Be careful, this will remove all keys.`\n        \"\"\"\n        with contextlib.suppress(FileNotFoundError):\n            await to_thread.run_sync(rmtree, constants.DB_ROOT)\n        return\n\n    @staticmethod\n    def _task_find(\n        key: int,\n        filter_fn: Callable,\n        length_reduction_hash: str,\n        db_root: str,\n        class_model: T,\n    ) -&gt; dict[str, Any] | None:\n        \"\"\"Task for searching for documents.\n\n        This method is for internal use.\n        \"\"\"\n        key_as_hash: str = f\"{key:08x}\"[length_reduction_hash:]\n        separated_hash: str = \"/\".join(list(key_as_hash))\n        leaf_path: SyncPath = SyncPath(\n            *(\n                db_root,\n                class_model.__name__,\n                separated_hash,\n                \"leaf.json\",\n            ),\n        )\n        if leaf_path.exists():\n            data_json: bytes = leaf_path.read_bytes()\n            data: dict[str, str] = orjson.loads(data_json) or {}\n            for _, val in data.items():\n                doc = class_model.model_validate_json(val)\n                if filter_fn(doc):\n                    return doc\n        return None\n\n    def find_one(\n        self,\n        filter_fn: Callable,\n        max_workers: int | None = None,\n        timeout: float | None = None,\n    ) -&gt; T | None:\n        \"\"\"Find a single document matching the filter.\n\n        The search is based on the effect of a quantum loop.\n        The search effectiveness depends on the number of processor threads.\n        Ideally, hundreds and even thousands of threads are required.\n\n        Args:\n            filter_fn: A function that execute the conditions of filtering.\n            max_workers: The maximum number of processes that can be used to\n                         execute the given calls. If None or not given then as many\n                         worker processes will be created as the machine has processors.\n            timeout: The number of seconds to wait for the result if the future isn't done.\n                     If None, then there is no limit on the wait time.\n        \"\"\"\n        keys: range = range(1, self.__max_num_keys)\n        search_task_fn: Callable = self._task_find\n        length_reduction_hash: int = self.__length_reduction_hash\n        db_root: str = self.__db_root\n        class_model: T = self.__class_model\n        with concurrent.futures.ThreadPoolExecutor(max_workers) as executor:\n            for key in keys:\n                future = executor.submit(\n                    search_task_fn,\n                    key,\n                    filter_fn,\n                    length_reduction_hash,\n                    db_root,\n                    class_model,\n                )\n                doc = future.result(timeout)\n                if doc is not None:\n                    return doc\n        return None\n\n    def find(\n        self,\n        filter_fn: Callable,\n        db_query_docs_limit: int = 1000,\n        max_workers: int | None = None,\n        timeout: float | None = None,\n    ) -&gt; list[T] | None:\n        \"\"\"Find one or more documents matching the filter.\n\n        The search is based on the effect of a quantum loop.\n        The search effectiveness depends on the number of processor threads.\n        Ideally, hundreds and even thousands of threads are required.\n\n        Args:\n            filter_fn: A function that execute the conditions of filtering.\n            db_query_docs_limit: Limiting the number of request results. By default = 1000.\n            max_workers: The maximum number of processes that can be used to\n                         execute the given calls. If None or not given then as many\n                         worker processes will be created as the machine has processors.\n            timeout: The number of seconds to wait for the result if the future isn't done.\n                     If None, then there is no limit on the wait time.\n        \"\"\"\n        keys: range = range(1, self.__max_num_keys)\n        search_task_fn: Callable = self._task_find\n        length_reduction_hash: int = self.__length_reduction_hash\n        db_root: str = self.__db_root\n        class_model: T = self.__class_model\n        counter: int = 0\n        with concurrent.futures.ThreadPoolExecutor(max_workers) as executor:\n            results = []\n            for key in keys:\n                if counter == db_query_docs_limit:\n                    break\n                future = executor.submit(\n                    search_task_fn,\n                    key,\n                    filter_fn,\n                    length_reduction_hash,\n                    db_root,\n                    class_model,\n                )\n                doc = future.result(timeout)\n                if doc is not None:\n                    results.append(doc)\n                    counter += 1\n        return results or None\n\n    def collection_name(self) -&gt; str:\n        \"\"\"Get collection name.\"\"\"\n        return self.__class_model.__name__\n\n    def collection_full_name(self) -&gt; str:\n        \"\"\"Get full name of collection.\"\"\"\n        return f\"{self.__db_root}/{self.__class_model.__name__}\"\n\n    async def estimated_document_count(self) -&gt; int:\n        \"\"\"Get an estimate of the number of documents in this collection using collection metadata.\"\"\"\n        meta = await self._get_meta()\n        return meta.counter_documents\n\n    def count_documents(\n        self,\n        filter_fn: Callable,\n        max_workers: int | None = None,\n        timeout: float | None = None,\n    ) -&gt; int:\n        \"\"\"Count the number of documents a matching the filter in this collection.\n\n        The search is based on the effect of a quantum loop.\n        The search effectiveness depends on the number of processor threads.\n        Ideally, hundreds and even thousands of threads are required.\n\n        Args:\n            filter_fn: A function that execute the conditions of filtering.\n            max_workers: The maximum number of processes that can be used to\n                         execute the given calls. If None or not given then as many\n                         worker processes will be created as the machine has processors.\n            timeout: The number of seconds to wait for the result if the future isn't done.\n                     If None, then there is no limit on the wait time.\n        \"\"\"\n        keys: range = range(1, self.__max_num_keys)\n        search_task_fn: Callable = self._task_find\n        length_reduction_hash: int = self.__length_reduction_hash\n        db_root: str = self.__db_root\n        class_model: T = self.__class_model\n        counter: int = 0\n        with concurrent.futures.ThreadPoolExecutor(max_workers) as executor:\n            for key in keys:\n                future = executor.submit(\n                    search_task_fn,\n                    key,\n                    filter_fn,\n                    length_reduction_hash,\n                    db_root,\n                    class_model,\n                )\n                if future.result(timeout) is not None:\n                    counter += 1\n        return counter\n</code></pre>"},{"location":"pages/db/#scruby.db.Scruby.collection_full_name","title":"<code>collection_full_name()</code>","text":"<p>Get full name of collection.</p> Source code in <code>src\\scruby\\db.py</code> <pre><code>def collection_full_name(self) -&gt; str:\n    \"\"\"Get full name of collection.\"\"\"\n    return f\"{self.__db_root}/{self.__class_model.__name__}\"\n</code></pre>"},{"location":"pages/db/#scruby.db.Scruby.collection_name","title":"<code>collection_name()</code>","text":"<p>Get collection name.</p> Source code in <code>src\\scruby\\db.py</code> <pre><code>def collection_name(self) -&gt; str:\n    \"\"\"Get collection name.\"\"\"\n    return self.__class_model.__name__\n</code></pre>"},{"location":"pages/db/#scruby.db.Scruby.count_documents","title":"<code>count_documents(filter_fn, max_workers=None, timeout=None)</code>","text":"<p>Count the number of documents a matching the filter in this collection.</p> <p>The search is based on the effect of a quantum loop. The search effectiveness depends on the number of processor threads. Ideally, hundreds and even thousands of threads are required.</p> <p>Parameters:</p> Name Type Description Default <code>filter_fn</code> <code>Callable</code> <p>A function that execute the conditions of filtering.</p> required <code>max_workers</code> <code>int | None</code> <p>The maximum number of processes that can be used to          execute the given calls. If None or not given then as many          worker processes will be created as the machine has processors.</p> <code>None</code> <code>timeout</code> <code>float | None</code> <p>The number of seconds to wait for the result if the future isn't done.      If None, then there is no limit on the wait time.</p> <code>None</code> Source code in <code>src\\scruby\\db.py</code> <pre><code>def count_documents(\n    self,\n    filter_fn: Callable,\n    max_workers: int | None = None,\n    timeout: float | None = None,\n) -&gt; int:\n    \"\"\"Count the number of documents a matching the filter in this collection.\n\n    The search is based on the effect of a quantum loop.\n    The search effectiveness depends on the number of processor threads.\n    Ideally, hundreds and even thousands of threads are required.\n\n    Args:\n        filter_fn: A function that execute the conditions of filtering.\n        max_workers: The maximum number of processes that can be used to\n                     execute the given calls. If None or not given then as many\n                     worker processes will be created as the machine has processors.\n        timeout: The number of seconds to wait for the result if the future isn't done.\n                 If None, then there is no limit on the wait time.\n    \"\"\"\n    keys: range = range(1, self.__max_num_keys)\n    search_task_fn: Callable = self._task_find\n    length_reduction_hash: int = self.__length_reduction_hash\n    db_root: str = self.__db_root\n    class_model: T = self.__class_model\n    counter: int = 0\n    with concurrent.futures.ThreadPoolExecutor(max_workers) as executor:\n        for key in keys:\n            future = executor.submit(\n                search_task_fn,\n                key,\n                filter_fn,\n                length_reduction_hash,\n                db_root,\n                class_model,\n            )\n            if future.result(timeout) is not None:\n                counter += 1\n    return counter\n</code></pre>"},{"location":"pages/db/#scruby.db.Scruby.delete_key","title":"<code>delete_key(key)</code>  <code>async</code>","text":"<p>Asynchronous method for deleting key from collection.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Key name.</p> required Source code in <code>src\\scruby\\db.py</code> <pre><code>async def delete_key(self, key: str) -&gt; None:\n    \"\"\"Asynchronous method for deleting key from collection.\n\n    Args:\n        key: Key name.\n    \"\"\"\n    # The path to the database cell.\n    leaf_path: Path = await self._get_leaf_path(key)\n    # Deleting key.\n    if await leaf_path.exists():\n        data_json: bytes = await leaf_path.read_bytes()\n        data: dict = orjson.loads(data_json) or {}\n        del data[key]\n        await leaf_path.write_bytes(orjson.dumps(data))\n        await self._counter_documents(-1)\n        return\n    msg: str = \"`delete_key` - The unacceptable key value.\"\n    logger.error(msg)\n    raise KeyError()\n</code></pre>"},{"location":"pages/db/#scruby.db.Scruby.estimated_document_count","title":"<code>estimated_document_count()</code>  <code>async</code>","text":"<p>Get an estimate of the number of documents in this collection using collection metadata.</p> Source code in <code>src\\scruby\\db.py</code> <pre><code>async def estimated_document_count(self) -&gt; int:\n    \"\"\"Get an estimate of the number of documents in this collection using collection metadata.\"\"\"\n    meta = await self._get_meta()\n    return meta.counter_documents\n</code></pre>"},{"location":"pages/db/#scruby.db.Scruby.find","title":"<code>find(filter_fn, db_query_docs_limit=1000, max_workers=None, timeout=None)</code>","text":"<p>Find one or more documents matching the filter.</p> <p>The search is based on the effect of a quantum loop. The search effectiveness depends on the number of processor threads. Ideally, hundreds and even thousands of threads are required.</p> <p>Parameters:</p> Name Type Description Default <code>filter_fn</code> <code>Callable</code> <p>A function that execute the conditions of filtering.</p> required <code>db_query_docs_limit</code> <code>int</code> <p>Limiting the number of request results. By default = 1000.</p> <code>1000</code> <code>max_workers</code> <code>int | None</code> <p>The maximum number of processes that can be used to          execute the given calls. If None or not given then as many          worker processes will be created as the machine has processors.</p> <code>None</code> <code>timeout</code> <code>float | None</code> <p>The number of seconds to wait for the result if the future isn't done.      If None, then there is no limit on the wait time.</p> <code>None</code> Source code in <code>src\\scruby\\db.py</code> <pre><code>def find(\n    self,\n    filter_fn: Callable,\n    db_query_docs_limit: int = 1000,\n    max_workers: int | None = None,\n    timeout: float | None = None,\n) -&gt; list[T] | None:\n    \"\"\"Find one or more documents matching the filter.\n\n    The search is based on the effect of a quantum loop.\n    The search effectiveness depends on the number of processor threads.\n    Ideally, hundreds and even thousands of threads are required.\n\n    Args:\n        filter_fn: A function that execute the conditions of filtering.\n        db_query_docs_limit: Limiting the number of request results. By default = 1000.\n        max_workers: The maximum number of processes that can be used to\n                     execute the given calls. If None or not given then as many\n                     worker processes will be created as the machine has processors.\n        timeout: The number of seconds to wait for the result if the future isn't done.\n                 If None, then there is no limit on the wait time.\n    \"\"\"\n    keys: range = range(1, self.__max_num_keys)\n    search_task_fn: Callable = self._task_find\n    length_reduction_hash: int = self.__length_reduction_hash\n    db_root: str = self.__db_root\n    class_model: T = self.__class_model\n    counter: int = 0\n    with concurrent.futures.ThreadPoolExecutor(max_workers) as executor:\n        results = []\n        for key in keys:\n            if counter == db_query_docs_limit:\n                break\n            future = executor.submit(\n                search_task_fn,\n                key,\n                filter_fn,\n                length_reduction_hash,\n                db_root,\n                class_model,\n            )\n            doc = future.result(timeout)\n            if doc is not None:\n                results.append(doc)\n                counter += 1\n    return results or None\n</code></pre>"},{"location":"pages/db/#scruby.db.Scruby.find_one","title":"<code>find_one(filter_fn, max_workers=None, timeout=None)</code>","text":"<p>Find a single document matching the filter.</p> <p>The search is based on the effect of a quantum loop. The search effectiveness depends on the number of processor threads. Ideally, hundreds and even thousands of threads are required.</p> <p>Parameters:</p> Name Type Description Default <code>filter_fn</code> <code>Callable</code> <p>A function that execute the conditions of filtering.</p> required <code>max_workers</code> <code>int | None</code> <p>The maximum number of processes that can be used to          execute the given calls. If None or not given then as many          worker processes will be created as the machine has processors.</p> <code>None</code> <code>timeout</code> <code>float | None</code> <p>The number of seconds to wait for the result if the future isn't done.      If None, then there is no limit on the wait time.</p> <code>None</code> Source code in <code>src\\scruby\\db.py</code> <pre><code>def find_one(\n    self,\n    filter_fn: Callable,\n    max_workers: int | None = None,\n    timeout: float | None = None,\n) -&gt; T | None:\n    \"\"\"Find a single document matching the filter.\n\n    The search is based on the effect of a quantum loop.\n    The search effectiveness depends on the number of processor threads.\n    Ideally, hundreds and even thousands of threads are required.\n\n    Args:\n        filter_fn: A function that execute the conditions of filtering.\n        max_workers: The maximum number of processes that can be used to\n                     execute the given calls. If None or not given then as many\n                     worker processes will be created as the machine has processors.\n        timeout: The number of seconds to wait for the result if the future isn't done.\n                 If None, then there is no limit on the wait time.\n    \"\"\"\n    keys: range = range(1, self.__max_num_keys)\n    search_task_fn: Callable = self._task_find\n    length_reduction_hash: int = self.__length_reduction_hash\n    db_root: str = self.__db_root\n    class_model: T = self.__class_model\n    with concurrent.futures.ThreadPoolExecutor(max_workers) as executor:\n        for key in keys:\n            future = executor.submit(\n                search_task_fn,\n                key,\n                filter_fn,\n                length_reduction_hash,\n                db_root,\n                class_model,\n            )\n            doc = future.result(timeout)\n            if doc is not None:\n                return doc\n    return None\n</code></pre>"},{"location":"pages/db/#scruby.db.Scruby.get_key","title":"<code>get_key(key)</code>  <code>async</code>","text":"<p>Asynchronous method for getting value of key from collection.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Key name.</p> required Source code in <code>src\\scruby\\db.py</code> <pre><code>async def get_key(self, key: str) -&gt; T:\n    \"\"\"Asynchronous method for getting value of key from collection.\n\n    Args:\n        key: Key name.\n    \"\"\"\n    # The path to the database cell.\n    leaf_path: Path = await self._get_leaf_path(key)\n    # Get value of key.\n    if await leaf_path.exists():\n        data_json: bytes = await leaf_path.read_bytes()\n        data: dict = orjson.loads(data_json) or {}\n        obj: T = self.__class_model.model_validate_json(data[key])\n        return obj\n    msg: str = \"`get_key` - The unacceptable key value.\"\n    logger.error(msg)\n    raise KeyError()\n</code></pre>"},{"location":"pages/db/#scruby.db.Scruby.has_key","title":"<code>has_key(key)</code>  <code>async</code>","text":"<p>Asynchronous method for checking presence of key in collection.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Key name.</p> required Source code in <code>src\\scruby\\db.py</code> <pre><code>async def has_key(self, key: str) -&gt; bool:\n    \"\"\"Asynchronous method for checking presence of key in collection.\n\n    Args:\n        key: Key name.\n    \"\"\"\n    # The path to the database cell.\n    leaf_path: Path = await self._get_leaf_path(key)\n    # Checking whether there is a key.\n    if await leaf_path.exists():\n        data_json: bytes = await leaf_path.read_bytes()\n        data: dict = orjson.loads(data_json) or {}\n        try:\n            data[key]\n            return True\n        except KeyError:\n            return False\n    return False\n</code></pre>"},{"location":"pages/db/#scruby.db.Scruby.napalm","title":"<code>napalm()</code>  <code>async</code> <code>staticmethod</code>","text":"<p>Asynchronous method for full database deletion.</p> <p>The main purpose is tests.</p> Warning <ul> <li><code>Be careful, this will remove all keys.</code></li> </ul> Source code in <code>src\\scruby\\db.py</code> <pre><code>@staticmethod\nasync def napalm() -&gt; None:\n    \"\"\"Asynchronous method for full database deletion.\n\n    The main purpose is tests.\n\n    Warning:\n        - `Be careful, this will remove all keys.`\n    \"\"\"\n    with contextlib.suppress(FileNotFoundError):\n        await to_thread.run_sync(rmtree, constants.DB_ROOT)\n    return\n</code></pre>"},{"location":"pages/db/#scruby.db.Scruby.set_key","title":"<code>set_key(key, value)</code>  <code>async</code>","text":"<p>Asynchronous method for adding and updating keys to collection.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Key name.</p> required <code>value</code> <code>T</code> <p>Value of key.</p> required Source code in <code>src\\scruby\\db.py</code> <pre><code>async def set_key(\n    self,\n    key: str,\n    value: T,\n) -&gt; None:\n    \"\"\"Asynchronous method for adding and updating keys to collection.\n\n    Args:\n        key: Key name.\n        value: Value of key.\n    \"\"\"\n    # The path to the database cell.\n    leaf_path: Path = await self._get_leaf_path(key)\n    value_json: str = value.model_dump_json()\n    # Write key-value to the database.\n    if await leaf_path.exists():\n        # Add new key or update existing.\n        data_json: bytes = await leaf_path.read_bytes()\n        data: dict = orjson.loads(data_json) or {}\n        if data.get(key) is None:\n            await self._counter_documents(1)\n        data[key] = value_json\n        await leaf_path.write_bytes(orjson.dumps(data))\n    else:\n        # Add new key to a blank leaf.\n        await leaf_path.write_bytes(orjson.dumps({key: value_json}))\n        await self._counter_documents(1)\n</code></pre>"},{"location":"pages/errors/","title":"Errors","text":"<p>XLOT Exceptions.</p>"},{"location":"pages/errors/#scruby.errors.MetadataValueError","title":"<code>MetadataValueError</code>","text":"<p>               Bases: <code>ScrubyException</code></p> <p>Exception is raised if value of variable in metadata does not matching expected.</p> Source code in <code>src\\scruby\\errors.py</code> <pre><code>class MetadataValueError(ScrubyException):\n    \"\"\"Exception is raised if value of variable in metadata does not matching expected.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:  # noqa: D107\n        self.message = message\n        super().__init__(self.message)\n</code></pre>"},{"location":"pages/errors/#scruby.errors.ScrubyException","title":"<code>ScrubyException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Root Custom Exception.</p> Source code in <code>src\\scruby\\errors.py</code> <pre><code>class ScrubyException(Exception):\n    \"\"\"Root Custom Exception.\"\"\"\n\n    def __init__(self, *args, **kwargs) -&gt; None:  # type: ignore[no-untyped-def]\n        super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"pages/installation/","title":"Installation","text":"<pre><code>uv add scruby\n</code></pre>"},{"location":"pages/usage/","title":"Usage","text":"<p>Example collection of how to use a database.</p>"},{"location":"pages/usage/count_documents/","title":"Count documents","text":"main.py<pre><code>\"\"\"Get an estimate of the number of documents in this collection using collection metadata.\"\"\"\n\nimport anyio\nimport datetime\nfrom typing import Annotated\nfrom pydantic import BaseModel, EmailStr\nfrom pydantic_extra_types.phone_numbers import PhoneNumber, PhoneNumberValidator\nfrom scruby import Scruby, constants\n\nconstants.DB_ROOT = \"ScrubyDB\"  # By default = \"ScrubyDB\"\n\nclass User(BaseModel):\n    \"\"\"Model of User.\"\"\"\n    first_name: str\n    last_name: str\n    birthday: datetime.datetime\n    email: EmailStr\n    phone: Annotated[PhoneNumber, PhoneNumberValidator(number_format=\"E164\")]\n\nasync def main() -&gt; None:\n    \"\"\"Example.\"\"\"\n    # Get collection of `User`.\n    user_coll = Scruby(User)\n\n    # Create user.\n    user = User(\n        first_name=\"John\",\n        last_name=\"Smith\",\n        birthday=datetime.datetime(1970, 1, 1),\n        email=\"John_Smith@gmail.com\",\n        phone=\"+447986123456\",\n    )\n\n    print(await user_coll.estimated_document_count())  # =&gt; 0\n\n    # Add user to collection.\n    await user_coll.set_key(\"+447986123456\", user)\n    print(await user_coll.estimated_document_count())  # =&gt; 1\n\n    # Delete user from collection.\n    await user_coll.delete_key(\"+447986123456\")\n    print(await user_coll.estimated_document_count())  # =&gt; 0\n\n    # Full database deletion.\n    # Hint: The main purpose is tests.\n    await Scruby.napalm()\n\nif __name__ == \"__main__\":\n    anyio.run(main)\n</code></pre> main.py<pre><code>\"\"\"Count the number of documents a matching the filter in this collection.\"\"\"\n\nimport anyio\nimport datetime\nfrom typing import Annotated\nfrom pydantic import BaseModel, EmailStr\nfrom pydantic_extra_types.phone_numbers import PhoneNumber, PhoneNumberValidator\nfrom scruby import Scruby, constants\n\nconstants.DB_ROOT = \"ScrubyDB\"  # By default = \"ScrubyDB\"\nconstants.LENGTH_REDUCTION_HASH = 6  # 256 branches in collection\n                                     # (main purpose is tests).\n\nclass User(BaseModel):\n    \"\"\"Model of User.\"\"\"\n    first_name: str\n    last_name: str\n    birthday: datetime.datetime\n    email: EmailStr\n    phone: Annotated[PhoneNumber, PhoneNumberValidator(number_format=\"E164\")]\n\nasync def main() -&gt; None:\n    \"\"\"Example.\"\"\"\n    # Get collection of `User`.\n    user_coll = Scruby(User)\n\n    # Create users.\n    for num in range(1, 10):\n        user = User(\n            first_name=\"John\",\n            last_name=\"Smith\",\n            birthday=datetime.datetime(1970, 1, num),\n            email=f\"John_Smith_{num}@gmail.com\",\n            phone=f\"+44798612345{num}\",\n        )\n        await db.set_key(f\"+44798612345{num}\", user)\n\n    result: int = user_coll.count_documents(\n        filter_fn=lambda doc: doc.email == \"John_Smith_5@gmail.com\" or doc.email == \"John_Smith_8@gmail.com\",\n    )\n    print(result:)  # =&gt; 2\n\n    # Full database deletion.\n    # Hint: The main purpose is tests.\n    await Scruby.napalm()\n\nif __name__ == \"__main__\":\n    anyio.run(main)\n</code></pre>"},{"location":"pages/usage/delete_documents/","title":"Delete documents","text":"main.py<pre><code>\"\"\"Find a single document and delete.\n\nThe search is based on the effect of a quantum loop.\nThe search effectiveness depends on the number of processor threads.\nIdeally, hundreds and even thousands of threads are required.\n\"\"\"\n\nimport anyio\nimport datetime\nfrom typing import Annotated\nfrom pydantic import BaseModel, EmailStr\nfrom pydantic_extra_types.phone_numbers import PhoneNumber, PhoneNumberValidator\nfrom scruby import Scruby, constants\n\nconstants.DB_ROOT = \"ScrubyDB\"  # By default = \"ScrubyDB\"\nconstants.LENGTH_REDUCTION_HASH = 6  # 256 branches in collection\n                                     # (main purpose is tests).\n\nclass User(BaseModel):\n    \"\"\"Model of User.\"\"\"\n    first_name: str\n    last_name: str\n    birthday: datetime.datetime\n    email: EmailStr\n    phone: Annotated[PhoneNumber, PhoneNumberValidator(number_format=\"E164\")]\n\nasync def main() -&gt; None:\n    \"\"\"Example.\"\"\"\n    # Get collection of `User`.\n    user_coll = Scruby(User)\n\n    # Create user.\n    user = User(\n        first_name=\"John\",\n        last_name=\"Smith\",\n        birthday=datetime.datetime(1970, 1, 1),\n        email=\"John_Smith@gmail.com\",\n        phone=\"+447986123456\",\n    )\n\n    # Add user to collection.\n    await user_coll.set_key(\"+447986123456\", user)\n\n    # Find user by email.\n    user_details: User | None = user_coll.find_one(\n        filter_fn=lambda doc: doc.email == \"John_Smith@gmail.com\",\n    )\n    # Delete user from collection.\n    await user_coll.delete_key(user_details.phone)\n\n    # Full database deletion.\n    # Hint: The main purpose is tests.\n    await Scruby.napalm()\n\nif __name__ == \"__main__\":\n    anyio.run(main)\n</code></pre>"},{"location":"pages/usage/find_a_single_document/","title":"Find a single document","text":"main.py<pre><code>\"\"\"Find a single document matching the filter.\n\nThe search is based on the effect of a quantum loop.\nThe search effectiveness depends on the number of processor threads.\nIdeally, hundreds and even thousands of threads are required.\n\"\"\"\n\nimport anyio\nimport datetime\nfrom typing import Annotated\nfrom pydantic import BaseModel, EmailStr\nfrom pydantic_extra_types.phone_numbers import PhoneNumber, PhoneNumberValidator\nfrom scruby import Scruby, constants\nfrom pprint import pprint as pp\n\nconstants.DB_ROOT = \"ScrubyDB\"  # By default = \"ScrubyDB\"\nconstants.LENGTH_REDUCTION_HASH = 6  # 256 branches in collection\n                                     # (main purpose is tests).\n\nclass User(BaseModel):\n    \"\"\"Model of User.\"\"\"\n    first_name: str\n    last_name: str\n    birthday: datetime.datetime\n    email: EmailStr\n    phone: Annotated[PhoneNumber, PhoneNumberValidator(number_format=\"E164\")]\n\nasync def main() -&gt; None:\n    \"\"\"Example.\"\"\"\n    # Get collection of `User`.\n    user_coll = Scruby(User)\n\n    # Create user.\n    user = User(\n        first_name=\"John\",\n        last_name=\"Smith\",\n        birthday=datetime.datetime(1970, 1, 1),\n        email=\"John_Smith@gmail.com\",\n        phone=\"+447986123456\",\n    )\n\n    # Add user to collection.\n    await user_coll.set_key(\"+447986123456\", user)\n\n    # Find user by email.\n    user_details: User | None = user_coll.find_one(\n        filter_fn=lambda doc: doc.email == \"John_Smith@gmail.com\",\n    )\n    if user_details is not None:\n        pp(user_details)\n    else:\n        print(\"No User!\")\n\n    # Find user by birthday.\n    user_details: User | None = user_coll.find_one(\n        filter_fn=lambda doc: doc.birthday == datetime.datetime(1970, 1, 1),\n    )\n    if user_details is not None:\n        pp(user_details)\n    else:\n        print(\"No User!\")\n\n    # Full database deletion.\n    # Hint: The main purpose is tests.\n    await Scruby.napalm()\n\nif __name__ == \"__main__\":\n    anyio.run(main)\n</code></pre>"},{"location":"pages/usage/find_documents/","title":"Find documents","text":"main.py<pre><code>\"\"\"Find one or more documents matching the filter.\n\nThe search is based on the effect of a quantum loop.\nThe search effectiveness depends on the number of processor threads.\nIdeally, hundreds and even thousands of threads are required.\n\"\"\"\n\nimport anyio\nimport datetime\nfrom typing import Annotated\nfrom pydantic import BaseModel, EmailStr\nfrom pydantic_extra_types.phone_numbers import PhoneNumber, PhoneNumberValidator\nfrom scruby import Scruby, constants\nfrom pprint import pprint as pp\n\nconstants.DB_ROOT = \"ScrubyDB\"  # By default = \"ScrubyDB\"\nconstants.LENGTH_REDUCTION_HASH = 6  # 256 branches in collection\n                                     # (main purpose is tests).\n\nclass User(BaseModel):\n    \"\"\"Model of User.\"\"\"\n    first_name: str\n    last_name: str\n    birthday: datetime.datetime\n    email: EmailStr\n    phone: Annotated[PhoneNumber, PhoneNumberValidator(number_format=\"E164\")]\n\nasync def main() -&gt; None:\n    \"\"\"Example.\"\"\"\n    # Get collection of `User`.\n    user_coll = Scruby(User)\n\n    # Create users.\n    for num in range(1, 10):\n        user = User(\n            first_name=\"John\",\n            last_name=\"Smith\",\n            birthday=datetime.datetime(1970, 1, num),\n            email=f\"John_Smith_{num}@gmail.com\",\n            phone=f\"+44798612345{num}\",\n        )\n        await db.set_key(f\"+44798612345{num}\", user)\n\n    # Find users by email.\n    users: list[User] | None = user_coll.find(\n        filter_fn=lambda doc: doc.email == \"John_Smith_5@gmail.com\" or doc.email == \"John_Smith_8@gmail.com\",\n    )\n    if users is not None:\n        pp(users)\n    else:\n        print(\"No users!\")\n\n    # Full database deletion.\n    # Hint: The main purpose is tests.\n    await Scruby.napalm()\n\nif __name__ == \"__main__\":\n    anyio.run(main)\n</code></pre>"},{"location":"pages/usage/get_collection_name/","title":"Get collection name","text":"main.py<pre><code>\"\"\"Get collection name.\"\"\"\n\nimport anyio\nimport datetime\nfrom typing import Annotated\nfrom pydantic import BaseModel, EmailStr\nfrom pydantic_extra_types.phone_numbers import PhoneNumber, PhoneNumberValidator\nfrom scruby import Scruby, constants\n\nconstants.DB_ROOT = \"ScrubyDB\"  # By default = \"ScrubyDB\"\n\nclass User(BaseModel):\n    \"\"\"Model of User.\"\"\"\n    first_name: str\n    last_name: str\n    birthday: datetime.datetime\n    email: EmailStr\n    phone: Annotated[PhoneNumber, PhoneNumberValidator(number_format=\"E164\")]\n\nasync def main() -&gt; None:\n    \"\"\"Example.\"\"\"\n    # Get collection of `User`.\n    user_coll = Scruby(User)\n\n    print(user_coll.collection_name())  # \"User\"\n    print(user_coll.collection_full_name())  # \"ScrubyDB/User\"\n\n    # Full database deletion.\n    # Hint: The main purpose is tests.\n    await Scruby.napalm()\n\nif __name__ == \"__main__\":\n    anyio.run(main)\n</code></pre>"},{"location":"pages/usage/working_with_keys/","title":"Working with keys","text":"main.py<pre><code>\"\"\"Working with keys.\"\"\"\n\nimport anyio\nimport datetime\nfrom typing import Annotated\nfrom pydantic import BaseModel, EmailStr\nfrom pydantic_extra_types.phone_numbers import PhoneNumber, PhoneNumberValidator\nfrom scruby import Scruby, constants\n\nconstants.DB_ROOT = \"ScrubyDB\"  # By default = \"ScrubyDB\"\n\nclass User(BaseModel):\n    \"\"\"Model of User.\"\"\"\n    first_name: str\n    last_name: str\n    birthday: datetime.datetime\n    email: EmailStr\n    phone: Annotated[PhoneNumber, PhoneNumberValidator(number_format=\"E164\")]\n\nasync def main() -&gt; None:\n    \"\"\"Example.\"\"\"\n    # Get collection of `User`.\n    user_coll = Scruby(User)\n\n    # Create user.\n    user = User(\n        first_name=\"John\",\n        last_name=\"Smith\",\n        birthday=datetime.datetime(1970, 1, 1),\n        email=\"John_Smith@gmail.com\",\n        phone=\"+447986123456\",\n    )\n\n    # Add user to collection.\n    await user_coll.set_key(\"+447986123456\", user)\n\n    # Get user from collection.\n    await user_coll.get_key(\"+447986123456\")  # =&gt; user\n    await user_coll.get_key(\"key missing\")  # =&gt; KeyError\n\n    await user_coll.has_key(\"+447986123456\")  # =&gt; True\n    await user_coll.has_key(\"key missing\")  # =&gt; False\n\n    await user_coll.delete_key(\"+447986123456\")\n    await user_coll.delete_key(\"+447986123456\")  # =&gt; KeyError\n    await user_coll.delete_key(\"key missing\")  # =&gt; KeyError\n\n    # Full database deletion.\n    # Hint: The main purpose is tests.\n    await Scruby.napalm()\n\nif __name__ == \"__main__\":\n    anyio.run(main)\n</code></pre>"}]}